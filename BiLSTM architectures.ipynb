{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# BiLSTM architectures for Sequence Labeling\n",
    "\n",
    "The objetive of this notebook is to build different BiLSTM prototype networks for sequence labeling, and apply a very simple attention mechanisms before the recurrent layer. The base model is inpired in [this work](https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm), by GaganBhatia. Most of the explanations of the code is in the accompaning slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n",
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libcrypto.so.1.0.0: no version information available (required by wget)\r\n",
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libssl.so.1.0.0: no version information available (required by wget)\r\n",
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libssl.so.1.0.0: no version information available (required by wget)\r\n",
      "File `data/data.sample.csv' already there; not retrieving.\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir data; wget -O data/data.sample.csv -nc https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gzip\n",
    "import keras\n",
    "import numpy\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "import seaborn\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dataset = pandas.read_csv(\"data/data.sample.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False,\n",
    "                          usecols=['sentence_idx', 'word', 'pos', 'tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let's take a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VBP</td>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBN</td>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TO</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos  sentence_idx           word    tag\n",
       "0  NNS             1      Thousands      O\n",
       "1   IN             1             of      O\n",
       "2  NNS             1  demonstrators      O\n",
       "3  VBP             1           have      O\n",
       "4  VBN             1        marched      O\n",
       "5   IN             1        through      O\n",
       "6  NNP             1         London  B-geo\n",
       "7   TO             1             to      O\n",
       "8   VB             1        protest      O\n",
       "9   DT             1            the      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class SentenceFactory(object):\n",
    "    \n",
    "    def __init__(self, dataset, tag_preprocess=lambda x: x):\n",
    "        self.dataset = dataset\n",
    "        agg_func = lambda s: [\n",
    "            (w, p, tag_preprocess(t)) \n",
    "            for w, p, t in zip(s[\"word\"].values.tolist(), s['pos'].values.tolist(),\n",
    "                               s[\"tag\"].values.tolist())\n",
    "        ]\n",
    "        grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Thousands', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('demonstrators', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('marched', 'VBN', 'O'),\n",
       "  ('through', 'IN', 'O'),\n",
       "  ('London', 'NNP', 'B-geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('protest', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('war', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Iraq', 'NNP', 'B-geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('demand', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('withdrawal', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('British', 'JJ', 'B-gpe'),\n",
       "  ('troops', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('that', 'DT', 'O'),\n",
       "  ('country', 'NN', 'O'),\n",
       "  ('.', '.', 'O')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances = SentenceFactory(dataset).sentences\n",
    "\n",
    "instances[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 10923\n"
     ]
    }
   ],
   "source": [
    "unique_words = dataset.word.unique()\n",
    "unique_words.sort()\n",
    "unique_words = numpy.append(unique_words, \"ENDPAD\")\n",
    "print('Vocabulary size {}'.format(unique_words.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '\"', '$', ..., '\\x85', '°C', 'ENDPAD'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'B-geo' 'B-gpe' 'B-per' 'I-geo' 'B-org' 'I-org' 'B-tim' 'B-art'\n",
      " 'I-art' 'I-per' 'I-gpe' 'I-tim' 'B-nat' 'B-eve' 'I-eve' 'I-nat']\n",
      "Unique labels 17\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.tag.fillna('O').unique()\n",
    "print(labels)\n",
    "print('Unique labels {}'.format(labels.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Processing the input sequences\n",
    "\n",
    "We need to pad our sequences to the same number of words, so the network can build a tensor from our (variable lenght) sentences. There are workarounds for this step, but maybe for another tutorial. In this case, we choose to use the lenght of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length = dataset.groupby('sentence_idx').word.count().max()\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(unique_words)}\n",
    "labels2idx = {t: i for i, t in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "x_matrix = [[word2idx[w[0]] for w in s] for s in instances]\n",
    "x_matrix = pad_sequences(maxlen=max_sentence_length, sequences=x_matrix,\n",
    "                         padding=\"post\", value=unique_words.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "y = [[labels2idx[w[2]] for w in s] for s in instances]\n",
    "y = pad_sequences(maxlen=max_sentence_length, sequences=y, padding=\"post\", value=labels2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=labels.shape[0]) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_matrix, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "--- \n",
    "\n",
    "# Building the model\n",
    "\n",
    "We build a model with an object oriented interface so we can add and remove layers in sub-classes. This is the vanilla model, which is only using the middle BiLSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import (Bidirectional, concatenate, Conv1D, Dense,\n",
    "                          Dropout, Embedding, GlobalMaxPooling1D, Input,\n",
    "                          LSTM, TimeDistributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class BaseBiLSTM(object):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50):\n",
    "        self.model = None\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.labels = labels\n",
    "        self.n_labels = labels.shape[0]\n",
    "    \n",
    "    def add_input_layer(self):\n",
    "        return Input(shape=(None, ))\n",
    "        \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = Embedding(\n",
    "            input_dim=self.vocabulary_size,\n",
    "            output_dim=self.embedding_size,\n",
    "            input_length=self.max_sentence_length)(layers)\n",
    "        return Dropout(0.1)(layers)\n",
    "    \n",
    "    def add_recurrent_layer(self, layers):\n",
    "        layers = Bidirectional(\n",
    "            LSTM(units=100, return_sequences=True,\n",
    "                 recurrent_dropout=0.1))(layers)\n",
    "        return layers\n",
    "    \n",
    "    def add_output_layer(self, layers):\n",
    "        layers = TimeDistributed(\n",
    "            Dense(self.n_labels, activation='softmax'), name='dense_layer')(layers)\n",
    "        return layers, 'categorical_crossentropy'\n",
    "    \n",
    "    def build(self):\n",
    "        inputs = self.add_input_layer()\n",
    "        layers = self.add_embedding_layer(inputs)\n",
    "        layers = self.add_recurrent_layer(layers)\n",
    "        outputs, loss_function = self.add_output_layer(layers)        \n",
    "        \n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer='adam', loss=loss_function,\n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs, batch_size=32, validation_split=0.2):\n",
    "        if self.model is None:\n",
    "            self.build()\n",
    "        return self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                              validation_split=validation_split, verbose=1)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return numpy.argmax(self.model.predict(X_test), axis=-1)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, cm=False):\n",
    "        predictions = numpy.argmax(self.model.predict(X_test), axis=-1).flatten()\n",
    "        true_labels = numpy.argmax(y_test, axis=-1).flatten()\n",
    "        print(metrics.classification_report(true_labels, predictions,\n",
    "                                            target_names=self.labels))\n",
    "        if cm:\n",
    "            seaborn.heatmap(\n",
    "                metrics.confusion_matrix(true_labels, predictions, labels=range(len(self.labels))),\n",
    "                xticklabels=self.labels, yticklabels=self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 70, 50)            546200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 70, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 70, 200)           120800    \n",
      "_________________________________________________________________\n",
      "dense_layer (TimeDistributed (None, 70, 17)            3417      \n",
      "=================================================================\n",
      "Total params: 670,417\n",
      "Trainable params: 670,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BaseBiLSTM(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=50)\n",
    "model.build()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 2s 29ms/step - loss: 2.8218 - acc: 0.3314 - val_loss: 2.7312 - val_acc: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1067bde80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=X_train[:100], y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "# Adding pre-trained word embeddings\n",
    "\n",
    "To preload the weights of a previous embedding, we only need to add an embedding layer with a pre-set weights. However, the indices of the tokens in our data won't be the same as the ones in the pretrained embeddings. As a result, we need a pretraining step where we map our training vocabulary.\n",
    "\n",
    "This process is pretty straight forward and there are several tutorials online that delve deeper into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocabulary, embeddings_filepath):\n",
    "    # TODO We are using the Konminos embeddings in part because they are easy to load\n",
    "    # but you should overwrite this part to fit every particular word embedding to load.\n",
    "    # Warning, this may consume your RAM, depedending on the type of embedding\n",
    "    # you are loading!\n",
    "    if vocabulary is None:\n",
    "        raise ValueError('vocabulary cannot be empty')\n",
    "    original_embeddings = gzip.open(embeddings_filepath, 'rt')\n",
    "    embedding_size = None\n",
    "    embeddings = None\n",
    "    not_found_words = 0\n",
    "    for line in original_embeddings:\n",
    "        split = line.rstrip().split(\" \")\n",
    "        word = split[0]\n",
    "        if embedding_size is None:  # Only runs first time\n",
    "            embedding_size = len(split) - 1\n",
    "            # We add one more place for the padding token\n",
    "            embeddings = numpy.zeros((len(vocabulary) + 1, embedding_size))\n",
    "\n",
    "        # Assure that all lines in the embeddings file are of the same length\n",
    "        if (len(split) - 1) != embedding_size:\n",
    "            print('ERROR: A line in the embeddings file had more or less '\n",
    "                  'dimensions than expected. Skip token.')\n",
    "            continue\n",
    "\n",
    "        if word in vocabulary:\n",
    "            vector = numpy.array([float(num) for num in split[1:]])\n",
    "            embeddings[vocabulary[word]] = vector  # Save the embedding in the\n",
    "                                                   # correct row\n",
    "        # If the word is not in vocabulary, we ignore the embedding.                \n",
    "    del original_embeddings\n",
    "    # Calculate which words are still missing\n",
    "    empty_words = numpy.where(embeddings.any(axis=1))[0]\n",
    "    print('{} words will be set with a random embedding')\n",
    "    # Replace missing words with a random vector (so far they are all zeros)\n",
    "    embeddings[empty_words] = numpy.random.uniform(-0.25, 0.25, embedding_size)\n",
    "\n",
    "    # Adding padding token at the end\n",
    "    embeddings[-1,:] = numpy.zeros(embedding_size)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the embeddings to avoid calculating them again. After the first time you run this cell, comment this two lines and read the embeddings from disk directly. It is very important to keep a copy of your original word2idx, in case you need to preprocess new data. In this case, we are using always the same dataset and ordering the tokens, so we shouldn't have inconsistencies preprocessing the text more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{} words will be set with a random embedding\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix = build_embedding_matrix(\n",
    "    vocabulary=word2idx,\n",
    "    embeddings_filepath='../../../am/data/embeddings/komninos_english_embeddings.gz')\n",
    "\n",
    "with open(os.path.join('models', 'komninos_word_embeddings.pickle'), 'wb') as file_:\n",
    "      pickle.dump([embeddings_matrix, word2idx], file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'komninos_word_embeddings.pickle'), 'rb') as file_:\n",
    "      embeddings_matrix, _ = pickle.load(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class WEmbBiLSTM(BaseBiLSTM):\n",
    "    \n",
    "    def add_word_embeddings(self, embeddings):\n",
    "        \"\"\"Saves into the model a matrix with the original weights for \n",
    "        the word embeddings.\n",
    "        \n",
    "        MUST BE CALLED BEFORE build, otherwise it has no effect.\n",
    "\n",
    "        Args:\n",
    "            embeddings: (numpy.ndarray) a 2-dimensional matrix with shape\n",
    "                (vocabulary_size, embedding_size).\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_size = embeddings.shape[1]  # Overwrite this value\n",
    "        self.vocabulary_size = embeddings.shape[0]  # Overwrite this value\n",
    "\n",
    "    def add_embedding_layer(self, layers):\n",
    "        if self.embeddings is not None:  # Add the pretrained embeddings\n",
    "            layers = Embedding(\n",
    "                input_dim=self.vocabulary_size, output_dim=self.embedding_size,\n",
    "                weights=[self.embeddings],\n",
    "                trainable=False, input_length=self.max_sentence_length)(layers)\n",
    "        else:  # We use brand new embeddings\n",
    "            layers = Embedding(\n",
    "                input_dim=self.vocabulary_size, output_dim=self.embedding_size,\n",
    "                input_length=self.max_sentence_length)(layers)\n",
    "        return Dropout(0.1)(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 70, 300)           3277200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 70, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dense_layer (TimeDistributed (None, 70, 17)            3417      \n",
      "=================================================================\n",
      "Total params: 3,601,417\n",
      "Trainable params: 324,217\n",
      "Non-trainable params: 3,277,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = WEmbBiLSTM(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=300)\n",
    "model.add_word_embeddings(embeddings_matrix)\n",
    "model.build()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23887191, -0.15608303,  0.02537438, ...,  0.06760797,\n",
       "         0.04049218, -0.22385186],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.23887191, -0.15608303,  0.02537438, ...,  0.06760797,\n",
       "         0.04049218, -0.22385186],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 2.6705 - acc: 0.5798 - val_loss: 2.3072 - val_acc: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0fc476828>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=X_train[:100], y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "# Adding character embeddings\n",
    "\n",
    "Before adding the character embeddings, we need to assign an index to each char.\n",
    "\n",
    "Now, the char input will be a 4-dimensional tensor, because each word is a list of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_len = max([len(word) for word in unique_words])\n",
    "max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the words that complicates everything are: ['Pakistani-controlled' 'bigger-than-expected' 'pro-U.S.-immigration'\n",
      " 'Chancellor-designate' 'government-controlled' \"Baku-T'bilisi-Erzerum\"\n",
      " 'multi-million-selling' 'counter-revolutionary' 'internationally-backed'\n",
      " 'internationally-recognized']\n"
     ]
    }
   ],
   "source": [
    "print('And the words that complicates everything are: {}'.format(\n",
    "    unique_words[numpy.argsort([len(word) for word in unique_words])[-10:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'62 unique characters used'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars, counts = numpy.unique([c for c in ''.join(unique_words)], return_counts=True)\n",
    "char_limit = 60\n",
    "char2idx = {c: idx + 1 for idx, c in enumerate(unique_chars[numpy.argsort(counts)[-1*char_limit:]])}\n",
    "char2idx['UNK'] = len(char2idx) + 1\n",
    "char2idx['PAD'] = 0  # This is not needed, it's only for clarification\n",
    "'{} unique characters used'.format(len(char2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def extract_chars(train_matrix):\n",
    "    char_matrix = numpy.zeros(train_matrix.shape + (max_word_len,) )\n",
    "    unknown_char_idx = len(char2idx) - 1\n",
    "    for example_idx, sentence in enumerate(train_matrix):\n",
    "        for token_position, word_idx in enumerate(sentence):\n",
    "            if word_idx == len(unique_words):  # Padding token\n",
    "                continue\n",
    "            chars = [char2idx.get(c, unknown_char_idx) for c in unique_words[word_idx]]\n",
    "            char_matrix[example_idx, token_position, :len(chars)] = chars\n",
    "    return char_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 70, 26)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chars(X_train[:5]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally build our model. We need to add one more input and the new character embeddings layer. We build a single model for both types of embeddings to ilustrate the fact that they share a lot of code, but ideally this should be implemented in different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class CEmbWEmbBiLSTM(WEmbBiLSTM):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50, max_char_index=50, char_embedding_type='cnn',\n",
    "                 max_word_length=20):\n",
    "        super(CEmbWEmbBiLSTM, self).__init__(vocabulary_size, max_sentence_length,\n",
    "                                             labels, embedding_size)\n",
    "        self.max_char_index = max_char_index\n",
    "        self.char_embedding_type = char_embedding_type\n",
    "        self.max_word_length = max_word_length\n",
    "    \n",
    "    def add_input_layer(self):\n",
    "        word_input = Input(shape=(self.max_sentence_length,), name='word_input')\n",
    "        char_input = Input(shape=(self.max_sentence_length, self.max_word_length), name='char_input')\n",
    "        return word_input, char_input\n",
    "    \n",
    "    def add_char_embedding_layer(self, char_input):\n",
    "        \"\"\"Add a convolution for the characters\"\"\"\n",
    "        limit = numpy.sqrt(3.0/self.max_char_index)\n",
    "        char_embedding_size = 15  # TODO define this as a parameter\n",
    "        # We initialize the char embeddings randomly, including the\n",
    "        # UNK char in position 0.\n",
    "        self.char_embeddings = numpy.random.uniform(\n",
    "            -limit, limit, (self.max_char_index + 1, char_embedding_size))\n",
    "\n",
    "        # We need the TimeDistributed layer to apply the embedding to every word\n",
    "        chars_layer = TimeDistributed(Embedding(\n",
    "            input_dim=self.char_embeddings.shape[0],\n",
    "            output_dim=self.char_embeddings.shape[1],\n",
    "            weights=[self.char_embeddings], trainable=True,\n",
    "            mask_zero=True), name='char_embedding')(char_input)\n",
    "\n",
    "        if self.char_embedding_type.lower() == 'lstm':\n",
    "            # Use LSTM for char embeddings from Lample et al., 2016\n",
    "            char_lstm_size = 10  # TODO define this as a parameter\n",
    "            chars_layer = TimeDistributed(Bidirectional(\n",
    "                LSTM(char_lstm_size, return_sequences=False)),\n",
    "                name=\"char_lstm\")(chars_layer)\n",
    "        elif self.char_embedding_type.lower() == 'cnn':\n",
    "            # Use CNNs for character embeddings from Ma and Hovy, 2016\n",
    "            char_filter_size = 5  # TODO define this as a parameter\n",
    "            char_filter_length = 5  # TODO define this as a parameter\n",
    "            chars_layer = TimeDistributed(\n",
    "                Conv1D(char_filter_size, char_filter_length, padding='same'),\n",
    "                name=\"char_cnn\")(chars_layer)\n",
    "            chars_layer = TimeDistributed(GlobalMaxPooling1D(),\n",
    "                                          name=\"char_pooling\")(chars_layer)\n",
    "        return chars_layer\n",
    "    \n",
    "    def add_embedding_layer(self, layers):\n",
    "        word_input, char_input = layers\n",
    "        # Add word embeddings with the previous method\n",
    "        word_layers = super(CEmbWEmbBiLSTM, self).add_embedding_layer(word_input)\n",
    "        \n",
    "        # Add char embeddings\n",
    "        char_layer = self.add_char_embedding_layer(char_input)\n",
    "        return concatenate([word_layers, char_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, 70, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_input (InputLayer)         (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, 70, 26, 15)   930         char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 70, 300)      3277200     word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn (TimeDistributed)      (None, 70, 26, 5)    380         char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 70, 300)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "char_pooling (TimeDistributed)  (None, 70, 5)        0           char_cnn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 70, 305)      0           dropout_3[0][0]                  \n",
      "                                                                 char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 70, 200)      324800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (TimeDistributed)   (None, 70, 17)       3417        bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 3,606,727\n",
      "Trainable params: 329,527\n",
      "Non-trainable params: 3,277,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CEmbWEmbBiLSTM(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=300,\n",
    "    max_char_index=max(char2idx.values()),  # The max possible id for a character\n",
    "    char_embedding_type='cnn', max_word_length=max_word_len)\n",
    "model.add_word_embeddings(embeddings_matrix)\n",
    "model.build()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 3s 41ms/step - loss: 2.7042 - acc: 0.4779 - val_loss: 2.3154 - val_acc: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0dc3ae550>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=[X_train[:100], extract_chars(X_train[:100])],\n",
    "          y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the other model now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CEmbWEmbBiLSTM(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=300,\n",
    "    max_char_index=max(char2idx.values()),  # The max possible id for a character\n",
    "    char_embedding_type='lstm', max_word_length=max_word_len)\n",
    "model.add_word_embeddings(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 5s 57ms/step - loss: 2.7267 - acc: 0.5718 - val_loss: 2.3560 - val_acc: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0d70efa20>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=[X_train[:100], extract_chars(X_train[:100])],\n",
    "          y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save('models/CEmbLSTMWEmbBiLSTM.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.build()\n",
    "model.model.load_weights('models/CEmbLSTMWEmbBiLSTM.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milagro/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.95      1.00      0.98      6663\n",
      "      B-geo       0.00      0.00      0.00        75\n",
      "      B-gpe       0.00      0.00      0.00        37\n",
      "      B-per       0.00      0.00      0.00        36\n",
      "      I-geo       0.00      0.00      0.00        14\n",
      "      B-org       0.00      0.00      0.00        47\n",
      "      I-org       0.00      0.00      0.00        35\n",
      "      B-tim       0.00      0.00      0.00        41\n",
      "      B-art       0.00      0.00      0.00         2\n",
      "      I-art       0.00      0.00      0.00        35\n",
      "      I-per       0.00      0.00      0.00        14\n",
      "      I-gpe       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.91      0.95      0.93      7000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEPCAYAAABMTw/iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm8XVV99/HPNwEkECFSkBcGEQxoRJQwSoQiIjIoAlqo\nUKgg1kAVxPI8tCI+xQFKKxaLUsWIEigzVCSlaECGihCGECIzEgaBVEHmQQZz7+/5Y69Ddi7n3LvP\n2fsM95zvm9d+5Zy119p7nYF111l77d9SRGBmZv1rQrcrYGZm7eWG3sysz7mhNzPrc27ozcz6nBt6\nM7M+54bezKzPuaE3M+tzbujNzPqcG3ozsz63Qrcr0Iw/PfFAqdt4J73lz6uqipn1uKWvLlHZYzTT\n5qy45ttLn69dxlVDb2bWUcND3a5BJdzQm5k1EsPdrkElujZGL2ldSZdIuk/S/ZJOlrRSt+pjZvY6\nw8PFtx7WlYZekoCfAD+NiI2AdwCTgeO7UR8zs3oihgtvvaxbPfodgZcj4nSAiBgC/g44WNIqXaqT\nmdny3KMv5d3ALfmEiHgOeBjYsCs1MjMbKYaLbz2s5+fRS5olaYGkBaedeW63q2Nmg2ToT8W3Htat\nWTd3AXvnEyStBqwHLM6nR8RsYDaUn0dvZtaUHh+SKapbPforgVUkfQpA0kTgX4E5EfHHLtXJzGw5\nvhhbQmQL1X4c2EfSfcBvgJeBL3ejPmZmdfXJxdiu3TAVEY8AH+vW+c3MxtTjPfWixtWdsatO3b7b\nVTCzQdLjF1mLGlcNvZlZR/X4kExRbujNzBrx0I2ZWZ8blB69pCHgdkDAEHBYRFzf7oqZmXVbFp1l\n/CvSo38pImYASNoFOAH4QFtrZWbWC/pk6KbZefSrAU/X2yFpmqQbJN0u6ThJL+T2HSXpZkm3Sfpa\nLv1ISXek7YutvQQzszYZWlp8K0DSFEkXSbpH0t2SZkpaQ9IVKWT7FZLelPJK0nckLU5t5+a54xyY\n8t8n6cCxzlukoZ8kaZGke4DTgG80yHcycHJEvAd4NFehnYGNgK2BGcAWkraXtAXwaeB9wDbAZyVt\nVueNeS3WzfDQiwWqa2ZWkeGh4lsxJwM/j4jpwKbA3cCXgCtTyPYr03OA3cjazo2AWcD3ASStARxL\n1nZuDRxb++PQSJGG/qWImJEqtitwZoonP9JM4ML0+Jxc+s5puxVYCExPFd8OuDgiXoyIF8ji079u\nUdeImB0RW0bElhMmrlqgumZmFakweqWk1YHtgR8BRMSrEfEMsCdwRsp2BrBXerwncGZkbgCmSFoH\n2AW4IiKeioingSvI2uaGmpp1ExHzJa0JrCXpCOCjKX3GaK8POCEifrBcYlbezKx3VTvrZgPgD8Dp\nkjYlC9V+BLB2RPwu5fk9sHZ6PBV4JFf+0ZTWKL2hpsboJU0HJgJPRsQxqadfa+RvAP4iPd43V2we\n2YIik9Mxpkp6M3AtsJekVSStShb75tpm6mNm1lZN9Ojzw8xpmzXiaCsAmwPfj4jNgBdZNkyTnS6L\nA1Z5lN4iPfpJkhalxwIOjPpzjr4InCXpGODnwLMAEXG5pHcB89OIzwvAARGxUNIc4KZU/rSIuLX1\nl2JmVrEmevT5kOoNPAo8GhE3pucXkTX0j0laJyJ+l4ZmHk/7lwBvzZVfN6UtAXYYkX7NaHUbs6GP\niIlj5clVapuICEn7Au/MHeNksosQI499EnBSweObmXVUVBjrJiJ+L+kRSe+MiHuBD5GtzXEXcCDw\nz+nfS1KRucBhks4ju/D6bPpjMA/4p9wF2J2Bo0c7d5V3xm4BnJIu1D4DHFzhsQHIftWYmXVI9XfG\nHg6cLWkl4AGymYcTgAskfQb4LfCXKe9lwEfIFmP6Y8pLRDwl6RvAzSnf1yPiqdFOqvHUeK640tRS\nlR0/r9TMylr66pJ6swOb8tLVpxVuNiZ98G9Kn69dHOvGzKyRQYl1Y2Y2sPokBIKDmpmZNVIwtEGv\nc1AzM7NG+mTopu1BzSTtIOmXkv5b0r2STpU0Ie3bWdJ8SQslXVi7qcrMrCf0yeLgbQ9qlmxNNq1o\nY2Aa8IkUSuErwE4RsTmwADhy5EGXC2o27KBmZtZBFca66aZmh25mkgU12yRePy9zJsuC8ZwDfCu3\n76aIeCAd41yygGYvkzX816U7ZlcC5o88ef5us7LTK83MmtLjPfWiOhHUDF4/hT3ILu5eERH7NVMH\nM7OO6fGeelGdCGoGsLWkDdLY/CeBX6X820raMB17VUnvaPWFmJlVruKFR7ql7UHNkpuBU4ANgavJ\n4tAPSzoIOFfSG1K+rwC/af5lmJm1waAM3VQR1Ax4LiJ2r3Psq4CtCh7fzKyzBqWhb0Lbg5qZmXXU\nOIoFNprKGvqIuJZsDcSR6dcwRqxkM7Oe5B69mVmfG7SG3jFvzGzg9PhsmqKa6dG3PeaNpBUioj/e\nWTMb//pkjL7ZWDc1o8W8mZPi2SyQ9BtJu6f0iZJOlHSzpNskHZLSd5B0raS5ZEtqmZn1hj6JddNM\nj742n35lYB1gx1Hyrk8W32YacHW6KepTZGsebpXmzV8n6fKUf3Ngk4h4sNkXYGbWNj3egBfV6tDN\naDFvAC6IiGHgPkkPANPJFrB9r6S9U57VgY2AV8li4dRt5CXNAmYBTJi4OhMmrNpElc3MSuiTEAgt\nzbopEPOmUWybwyNiXn6HpB2AhmEpHdTMzLolltYLAjD+tDRGP0bMG4B9JE2QNA14O3AvMA/4W0kr\npmO8Q5K752bWuwYoTHFN0Zg3AA8DN5FdtD00Il6WdBrZ2P3CdPfsH1gW1tjMrPcM98cgQuGGvomY\nNwC/iIhDR5QfBr6ctrxr8J2zZtaLBvBirJnZYHFDX19EHFT1MV87drsObGZWT5/cMOUevZlZI30y\n68YNvZlZIz0+m6aopht6SS9ExOR2VMbMrKcM2qwbM7NBE31yMbbVoGavI2mapBsk3S7pOEkv5PYd\nlQtm9rVc+pGS7kjbF6uqi5lZJYaj+NbDquzRnwycHBHnSnptDr2kncli2mxNdqPVXEnbk4U9+DTw\nvpR+o6T/iYhbK6yTmVnrBnWMfhQzWXan6znAt9LjndNWa8AnkzX8k4GLI+JFAEk/Af48l4+U/lpQ\nMzmomZl10qDPupF0PK8PZlY3K3BCRPxgRPkjipwnH9RsBQc1M7NO6vEhmaJaHqOvE8zsBuAv0uN9\nc1nnAQdLmgwgaaqkNwPXAntJWiUFN/t4SjMz6w0DGNRsLF8EzpJ0DPBz4FmAiLhc0ruA+VksM14A\nDoiIhZLmkAU/AzjN4/Nm1lP6pEffdEM/yhz6JcA2ERGS9gXemStzMtnF2pHHOgk4qdk6mJl1Qr9M\nr6yyR78FcEoKQfwMcHCFxzYz67ylbuiXExHXAptWdbx6VLJ8f/wIM7OO6fGx96J8Z6yZWSODOkZv\nZjYook8a+sLTKyUNSVok6deSFkp6fzsrZmbWdRWHQJA0UdKtki5Nz+dIejC1rYskzUjpkvQdSYtT\n6JjNc8c4UNJ9aTuwyHmb6dG/VJszL2kX4ATgA02Uf42kFSJiaStlzcw6pvpZN0cAd5Otp11zVERc\nNCLfbmQRBDYiCxPzfeB9ktYAjgW2JLvseIukuRHx9GgnbfWGqdWAugeWtL6kq9JfoSslrZfS50g6\nVdKNwDclrSXpCkl3SjpN0m8lrdlifczMqrd0uPg2BknrkkUTOK3AmfcEzozMDcAUSesAuwBXRMRT\nqXG/Ath1rIM109BPSj8t7kkV/UaDfN8FzoiI9wJnA9/J7VsXeH9EHEn2V+mqiHg3cBGwXr2DSZol\naYGkBcPDLzZRXTOzciKi8FbAvwF/D4z8q3B86hh/W9IbUtpU4JFcnkdTWqP0UTXT0L+UQh5MJ/sL\ncmaaMz/STLKgZgD/AWyX23dhRNSiBG0HnAcQET+nwS+EiJgdEVtGxJYOaGZmHdXEGH2+U5q2WbXD\nSNodeDwibhlxhqOB6cBWwBrAP7TjZbQ06yYi5qdhlrVScLIiwc0gC01sZjY+NDHrJh+AsY5tgT0k\nfQRYGVhN0lkRcUDa/4qk04H/m54vAd6aK79uSlsC7DAi/Zqx6tbSGL2k6cBE4Mk6wc2uZ1lQs/1p\nHKjsOuAv0/F2Bt7USl3MzNolhqPwNupxIo6OiHUjYn2y9vGqiDggjbuTRkf2Au5IReYCn0qzb7YB\nno2I35EFidxZ0pskvYksBPy8sV5HMz36SZIWpccCDswNw+QdDpwu6SjgD2SLi9TzNeBcSX8NzAd+\nDzzfRH3MzNqr/fPoz5a0FlmbugioLdp0GfARYDHwR1I7GhFPSfoGcHPK9/WIeGqsk6jgRYTKpYsO\nQxGxVNJM4PtjDf2sWDIefX/c+mBmRSx9dUnZqCk8s/+OhZuNKWdfVfp87dLNO2PXAy6QNAF4Ffhs\nF+tiZvZ6fXJnbNca+oi4D9isW+c3MxtTf8Q0c6wbM7NG+iXWjRt6M7NGBr1HL+mFUVabMjMb92Kp\ne/Qtc1AzMxsP+mTdkeobeknrAz8G1iTNo4+Ih9NC4C+TXYC9TtIJZKES3kI2j/7DwBYR8UTVdTIz\na0mfNPStRq8cjYOamVlfiOHiWy9rR0PvoGZm1h+Gm9h6WOmhG0nH46BmZtaHer2nXlTpHr2DmplZ\nvxpeWnzrZe0Yujkc+LSk24C/Jls6q56vkUVhuwPYBwc1M7NeEyq+9bCWh24azaGPiN8CO9ZJP2hE\n0rPALrmgZltFxCut1sfMrGr9MnTjoGZmZg3EcG/31ItyUDMzswbcozcz63PDQ+7Rm5n1tX4Zumlq\n1o2kIUmLJP1a0kJJ72+Qby9JG+eef13STmUra2bWSRHFt17WbI/+pdp8eUm7ACcAH6iTby/gUuAu\ngIj4xzKVNDPrhoHs0Y+wGnXCFqRe/h7Aian3P03SHEl7p/0PSToh7VsgaXNJ8yTdL+nQkcczM+uW\nGFbhrZc126OfJGkRsDKwDvXny18vaS5waURcBCC97k14OCJmSPo2MAfYNh3zDuDUfEZJs4BZABMm\nro7j3ZhZp/T6kExRZYZuZgJnStokoum3Y27693ZgckQ8Dzwv6RVJUyLimVrGiJgNzAZYcaWpffK2\nm9l4MDzUjuABnVfmztj5ktYE1pJ0BMUDmwHU7oAdzj2uPfdMIDPrCQM/j17SdGAi8GREHAMck9v9\nPPDGknUzM+uq4R6PYVNUq2P0AAIOzMWXzzsP+KGkLwB7l6mgmVm3RJ809Gp+eL17yo7Rj59XamZl\nLX11SelW+p53fKRwszH9N5f17F+FcTUe7obazDppHPWDRzWuGnozs04aGvRZN2Zm/a5fxuhb+nNV\nNOZNE8ebIekjZY5hZla1QY11U1M05s2YJK0AzAC2BC5rsT5mZpUb1OmV9dSNeQMg6WPAV4CVgCeB\n/SPiMUlfBaYBbwceJguBMEnSdsAJEXF+BfUyMyulX4ZuWm3ox4x5k/wK2CYiQtLfAH8P/J+0b2Ng\nu4h4SdJBwJYRcViL9TEzq9xQjwcrK6qKoZvRYt6sC5wvaR2yXv2DuX1zI+KlsU6UD2omBzUzsw7q\nlx596blDETEfqMW8OT5dpK3dPftd4JSIeA9wCNkvgJoXCx5/dkRsGRFbupE3s04aDhXeelnphn5k\nzJuImJELbLY6sCQ9PnCUwzg2jpn1nGhi62WtNvSTcj3382kc8+arwIWSbgGeGOV4VwMbp2N+ssU6\nmZlVql969C2N0UfExIL5LgEuqZP+1RHPnwK2aqUuZmbt0i9j9L4z1sysgSHc0JuZ9bXhXh98L6g/\nIvaYmbXBMCq8jUXSypJuSqFj7pT0tZS+gaQbJS2WdL6klVL6G9LzxWn/+rljHZ3S703RCUblht7M\nrIFAhbcCXgF2jIhNycK+7CppG+BfgG9HxIZkUQY+k/J/Bng6pX875UPSxsC+wLuBXYHvSRr1umkl\nDb2kF0qWd1AzM+s5w01sY4lMra1cMW1BFlngopR+BrBXerxnek7a/yFJSunnRcQrEfEgsBjYerRz\nd71Hnwtq5obezHpKxT16JE1M09IfB64A7geeiYilKcujwNT0eCrwCEDa/yzwZ/n0OmXqauvFWAc1\nM7PxbOnYWV6TD9eSzI6I2fk86X6jGZKmABcD08vXcmztnnXjoGZmNm4V7alDFq4FmD1mxizvM5Ku\nBmYCUyStkHrt67IsmsAS4K3Ao2nkY3WyDnMtvSZfpq52D92sC8yTdDtwFNnFg5rCQc0kLZC0YHi4\nUHgcM7NKDKv4NhZJa6WePJImAR8G7iaLDLB3ynYgy24yncuy0DF7A1elwJFzgX3TrJwNgI2Am0Y7\nd6U9eknHAx8FSPFuvgucFBFzJe1AFhKhpnBQM9JfyRVWmtons1rNbDwoMm2yCesAZ6QZMhOACyLi\nUkl3AedJOg64FfhRyv8j4D8kLQaeIptpQ0TcKekC4C6y0aXPNwhB85pKG/qIOAY4JpfkoGZmNm5V\n2bOMiNuAzeqkP0CdWTMR8TKwT4NjHQ8cX/Tc7R66+SoOamZm49RSqfDWyyrp0UfE5AbpDmpmZuNW\nv4wVO9aNmVkDRW6EGg/GVUNf9sdRv/x1NrPO6JMlY8dXQ29m1kkVz7rpGjf0ZmYN9MsoQMsNvaQX\nGl2ENTPrB0v7o0PfWz363G3AZmZd1y89+srn0UuaI+nUFLbgN5J2T+kTJZ0o6WZJt0k6JKXvIOla\nSXPJ7vQyM+sJVYZA6KZ29ejXJ7vTaxpwtaQNgU8Bz0bEVpLeAFwn6fKUf3NgkxRbeTn5iHATJq7O\nhAmrtqnKZmbL8/TK0V0QEcPAfZIeIAvFuTPwXkm14D2rkwXjeRW4qV4jD8vHulnRsW7MrIPc0Cd1\nApnB64e2gmwa/OERMW9E+R0oGODMzKyToseHZIoqPUYfEcdExIxcIw+wj6QJkmqLi9wLzAP+VtKK\nAJLeIcnjMGbWs5Y2sfWydg3dPEwWH3k14NCIeFnSaWRj9wvTuod/YNnaiGZmPadfxopbbujHmEP/\ni4g4dET+YeDLacu7Jm1mZj2l12fTFNVT8+jH0i9/Xc1sfPDF2AYi4qCqj2lm1g1u6M3M+tyQh27M\nzPpbv/Tom55eKemFdlTEzKzXRBNbL3OP3sysgeGeb8KLqSyomaRpkm6QdLuk42o9/xS07JeS/lvS\nvSng2YS0b2dJ8yUtlHShJIc9NrOeMdzE1suqjF55MnByRLwHeHTEvq2Bw4GNyQKdfULSmsBXgJ0i\nYnNgAXDkyINKmpUiYS4YHnakBDPrHA/dvN5Mlt3peg7wrdy+myLiAQBJ5wLbAS+TNfzXZTfKshIw\nf+RB80HNVnBQMzProIFfeKRBMLNGGgU5uyIi9mu1DmZm7TTwY/R1gpndAPxFerzviOxbS9ogjc1/\nEvhVyr9tilWPpFUlvaPV+piZVa1fhm6qHKP/InCkpNuADYFnc/tuBk4B7gYeBC6OiD8ABwHnpjLz\nyeLWm5n1hH65GNv00M0owcyWANtEREjaF3hnbt9zEbF7nWNdBWzVbB3MzDqhX4ZuqrwYuwVwSgpB\n/AxwcIXHNjPruKFuV6AilTX0EXEtsGmd9GtwGGIzG4fcozcz63P90cy7oTcza6jXL7IWVWrWTaMA\nZ5L2krRx7vnXJe1U5lxmZp0WTfzXy9rVo98LuBS4CyAi/rFN5zEzaxv36BuQ9H5gD+BESYtSsLM5\nkvZO+x+SdELat0DS5pLmSbpf0qGjH93MrHOGiMJbL2vHUoLXS5oLXBoRFwGkWDZ5D0fEDEnfBuYA\n2wIrA3cAp+YzSpoFzALQxNWZMGHVqqtsZlaXZ92UMzf9ezswOSKeB56X9IqkKRHxTC2jg5qZWbd4\n6CZH0vFpKGZRwSKvpH+Hc49rzz0TyMx6Qr9cjK2koa8T4Ox54I1VHNvMrFv6JdZN5Rdjk/OAoyTd\nKmlam85hZtZW7tHTOMBZRFwXERtHxGYRcX9EHFS7MBsR60fEE+nxnIg4LFfutX1mZt22NKLwNhZJ\nP5b0uKQ7cmlflbSkNvQt6SO5fUdLWpyWYN0ll75rSlss6UtFXke7evRmZuNexfHo5wC71kn/dm3o\nOyIuA0g3nO4LvDuV+Z6kiZImAv8O7Ea2Qt9++ZtTG/GFTzOzBqqcXhkRv5S0fsHsewLnRcQrwIOS\nFpOtvQ2wOLc063kp712jHcw9ejOzBjo0Rn+YpNvS0M6bUtpU4JFcnkdTWqP0UTXV0EsaSuNIv5a0\nMN0FW5qkKZI+V8WxzMyq0sysG0mz0t3+tW1WgVN8H5gGzAB+B/xr9a+i+aGbl2pTKNPFgROAD1RQ\njynA54DvVXAsM7NKDDUxcTJ/c2cTZR6rPZb0Q7IYYZCt2PfWXNZ1UxqjpDdUZuhmNeDpejtSbJvv\nSLpe0gO5ODeTJV2Zfg3cLmnPVOSfgWnp18KJJepkZlaZds+jl7RO7unHycLAQBY9YF9Jb5C0AbAR\ncBPZ+tsbSdpA0kpkF2znMoZme/ST0t2vKwPrADuOkncdYDuyBb/nAhcBLwMfj4jnJK0J3JDi4nwJ\n2CR3w5WZWddFgWmTRUk6F9gBWFPSo8CxwA6SZpBN3HkIOCSd905JF5BdZF0KfD4ihtJxDgPmAROB\nH0fEnWOdu8zQzUzgTEmbRP1346cRMQzcJWnt2msF/knS9mR/BKcCa9cp+xoHNTOzbql41s1+dZJ/\nNEr+44Hj66RfBlzWzLlbnl4ZEfNTr3wtSUcAH03ptV55PoZNLXzl/sBawBYR8SdJD5H9OhjtPA5q\nZmZd0euhDYpqeYxe0nSynw5P1ol108jqwOOpkf8g8LaU7tg4ZtZz+iUEQqtj9JD10g+sjRsVdDbw\nX5JuBxYA9wBExJOSrku3Bv8sIo5qsl5mZpUbiv7o0zfV0EfExIL5DhrxfHL69wlgZoMyf9VMXczM\n2q0/mnmHQDAza6jXh2SKckNvZtaAlxI0M+tzVc6j7yY39GZmDQxUj17SENlC3gKGgMMi4vp2VszM\nrNsGbdZNu4KZmZn1rP7oz7d2w9RowczWkvSfkm5O27aSJkh6SNKUXL77JK1dL3+rL8TMrGrDROGt\nlxXt0RcNZnYy2bJYv5K0HjAvIt4l6RKyyGynS3of8NuIeEzSOSPzA+8q9YrMzCrS6w14Ua0M3YwW\nzGwnYGOpFtqG1SRNBs4H/hE4nSys5vmj5Y+IF2oJDmpmZt3SL7NuVOSFSHqhdndrev4Y8B5guWBm\nkp4A1o2Il0eUF3Af2V2xNwFbprAHdfM34qBmZlbU0leXaOxco9vqLdsXbnNu/t9flj5fuzQ9Rj9G\nMLPLgcNzeWcApJ7/xcBJwN0R8eRo+c3MekFEFN56WbNj9DB6MLMvAP8u6bZ07F8Ch6Z955OtjnJQ\nwfxmZl01UGP0TQQzewL4ZIN9C1gWl37M/GZm3dbrPfWifGesmVkDA9WjNzMbRI5eaWbW5wYtBIKZ\n2cAZHuQx+pHz6s3M+pGHbszM+ly/9OhbCWrWkIOamVk/iSb+62VV9+grD2rmWDdm1i390qOvuqGv\nPKhZRMwGZoNj3ZhZZw3XDQAw/pRq6CUdTy6oGdlQ0DZ1gprNBzaUtBawF3Bc2lU3v5lZL+iXG6ZK\njdE7qJmZ9bN+CWpW6cVYsiBlW0q6TdJdLB+g7HzgAJYN24yV38ysq/plhalC8eh7hcfozayoKuLR\nT33Tuwu3OUuevrNn49F7Hr2ZWQMOgWBm1ufG04jHaNzQm5k10Otj70W5oTcza6BfevSlZt1IemHs\nXIWOM0XS56o4lplZVYYjCm+9rOrpla2aArihN7Oe4nn0o5A0R9J3JF0v6QFJe6f0yZKulLRQ0u2S\n9kxF/hmYJmmRpBPbUSczs2YNxXDhrZe1c4x+HWA7YDowF7gIeBn4eEQ8J2lN4AZJc4EvAZvk7rB9\njYOamVm39PqQTFHtbOh/GhHDwF2S1k5pAv5J0vbAMDAVWLvRAcBBzcyse3o9/HBRlTT0dYKbAbyS\nz5L+3R9YC9giIv4k6SFg5SrqYGZWtX7p0VcyRl8nuFkjqwOPp0b+g8DbUvrzwBurqIuZWVWqvhgr\naVdJ90paLOlLba7+azo96+ZssiBmtwOfAu4BSNEsr5N0hy/GmlmvGI7hwttYJE0E/h3YDdgY2E/S\nxm1+CUDJoZtGC4RHxEH18kXEE8DMBmX+qkxdzMyqVvG0ya2BxRHxAICk84A9gbuqPEk9vTKP3sys\n50QTWwFTgUdyzx9Nae3XzBhUr2/ArEEtP57r7tfu196Nc1e9kU0DX5DbZo3YvzdwWu75XwOndKJu\n/dajnzXA5cdz3cuWH891L1t+PNe9bPmy565URMyOiC1z2+wRWZYAb809XzeltV2/NfRmZr3qZmAj\nSRtIWgnYl+xm0rZz9Eozsw6IiKWSDgPmAROBH0fEnZ04d7819CN/Kg1S+fFc97Llx3Pdy5Yfz3Uv\nW77suTsuIi4DLuv0ecfVmrFmZtY8j9GbmfU5N/RmZn1u3I/RS1oZ2DA9XRwRL3ezPmZmvWbc9ugl\nrSDpm2R3l50BnAk8IumbklZs8libSjosbZs2WXYVSf9P0g/T840k7V6w7ERJ9zRzvjrHKFP3Neps\nhd+7suVzx1ktf4yCZfYpktaO8hV9bi1/b3LHaPp9S+X+pUhageOs0myZsuWrqvugGbcNPXAisAaw\nQURsERGbA9PIliX8VtGDSDqCLNjam9N2lqTDm6jH6WQhmWsxfJYAxxUpGBFDwL2S1mvifK+poO4L\ngT8AvwHuS48fSiuAbdHu8pIOkfR74DbglrQtKFj3owumVV6+7OeWtPy9Kfm+AXy4TtpuRQtLer+k\nu0hBCVNn43sdKl+q7gOr27cNl7jd+D7SrKER6ROB+5o4zm3AqrnnqwK3NVF+Qfr31lzar5so/0uy\nMM1Xkt08MReY26G6/xDYJfd8Z+AHwDbAjR0ofx+wZpOf+27Ad4HHgO/ktjnATe0uX8XnVvZ708r7\nlsr9LXA78GL67tS2B4GzmjjOjWR3eObrfkc7y1dV90HdxvMYfUT6BoxIHJLUzJxRAUO550MsWyil\niFclTSIvOJUBAAAJDUlEQVTFNZI0jeUXXRnL/2si70hl675NRHy29iQiLpf0rYg4RNIbOlD+fuCP\nTdQX4H/Jeq97kPVka54H/q4D5WvKfG5Q7nvTyvsGcA7wM+AEsuU7a56PiKeaOVBEPCIt91UbapS3\novKV1X0QjeeG/i5Jn4qIM/OJkg4g/SQs6HTgRkkXp+d7AT9qovyxwM+Bt0o6G9gWOKho4Yj4H0lv\nAzaKiF+kccuJBYuXrfvvJP0DcF56/kngsRQ3u8hqx2XLHw1cL+lGco1cRHyhUYGI+LWkO8h+SZxR\n4ByVls8dp8znBuW+N02/b2n/s8CzwH4Akt5MtsLbZEmTI+Lhgud/RNL7gUjXZI4A7i5YtqXyFdZ9\nII3bG6YkTQV+ArzEsp7ZlsAksgXICwcLkrQ52ULmANdGxK1N1uXPyIYrBNwQWdz9omU/SxacaY2I\nmCZpI+DUiPhQu+uubIH2Y1P5AK4Dvk72P9R6EbG4zeVvAn5F9pP8tT8MRRpgSdcCH4qIV8fK26by\npT63dIyWvjdl3rdU/mPAScBbgMfJVnq7OyLeXbD8msDJwE6p7pcDR0S2gFBby5et+6Aatz361JC/\nT9KOQO1DviwirmzhcKuQ/QQ8XdJakjaIiAebKP8BljV2KwIXj559OZ8nW5DgRoCIuC/1Vopqqe6p\n1/2liGh08XasRrpU+WTFiDiyQL56HiRblWwu2bgtABFxUofKl/3coPXvTZn3DbKLvtsAv4iIzZQt\n63lAE+UVEfuXOH+Z8mXrPpDGbUNfExFXAVe1Wl7SsWS/BN5JNhSyInAW2U/pIuW/RzaP/9yUdIik\nnSLi8wWr8EpEvFobr5S0AgXXMShT93QtY7ux8rWrfPIzSbOA/2L5IYgiY673p20Cra03XLZ8y59b\nyl/me1PmfQP4U0Q8KWmCpAkRcbWkfytad7I/kA8B5wP/GRHPNFG2bPmydR9I43bopiqSFgGbAQsj\nYrOUdltEvLdg+XuAd9UuDEuaANwZEe8qWP6bwDNka+geDnwOuCsijulA3b9PtsLNhSzfq/1Jh8rX\n++UREfH2IuW7qcznlsq3/L0p+75J+gXZ9ZwTgDXJhkC2ioj3FymfjrE1WZjdvciWwjsvIs5qd/kq\n6j6I3NBLN0XE1pIWRsTmklYF5jfRWF4KfD4ifpuev41s1ZiPFSw/AfgM2dREkYUwPa3ejKI21P30\nOskREQd3onwZktYC/p5s2G7l3Ml37FD5lj+3VL7U96aM9D15maze+wOrA2cXHWMfcaw1ycbM94+I\nZi5Gt1S+yroPknE/dFOBCyT9AJiSLrAdTDY/vKg3AnenC2RBNm67II39EhF7jFY4IoYlnUE21hvA\nvUUbi7J1j4hPF81bZXlJO0bEVZI+0eC4RX4RnE3203934FDgQLIbtooqVb7k5wYtfG8qet+IiBdz\nT5ueeSRpNeDjZD3yaWTXFrbuRPmydR9UA9+jB5D0YXI9s4i4oomyHxhtf0T8zxjlPwqcSjZeLGAD\n4JCI+FnB85ep+7pkNw/VxvSvJZv98Gg7y0v6WkQcW+YXgaRbImKL/FCVpJsjYquCdS9bvuzn1vT3\npor3LR3nE8C/kN1NrbRFRKxWsPyDwE+BCyJifpEyVZUvW/eBFT1w19Ygb2Rz/jfMPZ8G3NOhc18B\nfJrsl90KZPO4r+hg+Q2KpDUoe0P6dx7wUbJrFfc3ce6y5bv5ubX8vqW8i8muD7R6/loHcZVOly9b\n90Hdul6Bbm9kd0Q+N2J7hOzn5Ns7UP7mEc81Mq2N515UJK2N5RfWSbulYNndycZnNwGuJruXYo8m\nzl22fMufW9nPrsz7lvJeVzRvg/IzyS6gPpyebwp8rxPly9Z9UDeP0cO/kUXAPIfsf9bauOFC4MfA\nDm0uv0DSZcAFZGO1+wA318ZhY/Rx17LnflLZncS1KX77Ac1c1GqpvKTpZBdBVx8x3rwauQujo4mI\nS9PDZ4EPFq5xReUp97lBC59dFe9bru7nkw2f5KdnFhrjT3XfhbSwdWR3G2/fxPnLlC9b94E08GP0\nkn4dEZuOSFsUETPq7WtD+XrjrTURo4y7VnDut5GNsdciKF4HfCEK3k7eanlJe5JNkduD9D978jzZ\nNLvri5w/d7yFkUUvbUkr5ct8bql8059dVe9bBWP8N0bE+yTdGsum9Y75fauifDdneo1n7tHDHyX9\nJXBRer432fQtKHYDTKnyUW7mS9lz/5as0WhJq+Uj4hLgEkkzo4WLeXU0E8itkvIlPzdo4bOr6n2r\noO4dj3VTU0HdB1O3x466vQFvJ7vD8Im0/RfZHYuTgO3aXX7EsV439tqr5+6l8sBxJc9dtnzTda/q\ns+vG+052o9LZZKGeHye7G/vPOlW+qtc+SNvAD930kvxP2fF27m6WTzfdPBld+jIP8ufWTeO57p02\nnleYqpykhd0sD/z3eDx3J8tL2kbSNZJ+ImkzZSGH7yALj7xrgfLPS3quzva8pOfaWfdR6lTms+vq\n59bl/2fKvvaB4R59Trd7R2V6pt08dxWKnl/SAuDLZFMjZwO7RcQNaUbKueOxh9flXwSlPvdu/z9j\nxbhHv7yO9Y7K9ky7ee6yveKS518hIi6PiAuB30fEDQARUWqx7qK6+Yugy+97qbqXLd+m931wdPsi\nQS9tZBeJXrcObTvKky1ntzPZ/OunyZblA5hObi3NNr3Orp277PnJXYBjxMW4kc+9tf9zL/v/jLcO\nffbdrkDXXni2eME1ZKtUbUbWs/k92SyAXTtQflHu8d0j9o3V2NW7q/K5Wno7z13Re1/mtQ/lXuvS\nEa/9T93+XhV47aU+u2697ylP2e981177oG+DPI/+FJaN9V7FiLFesvU821k+v6bqSyP2jTpeGhGt\nLJRRybkrUua1Nx0Kt5dU8NmVUfZzL/Wd7/JrH2gDezG2dhdienx35BZ8KHKBqILyQ2SLdYhs7vQf\na7uAlSNixVZeVxHdPHcvnH9QlX3fy37nrXsGuUdftndTqnw3e6bd7hV3+/yDqoL3vdu/BK1Fg9yj\nL9u7ca/UBoq/8+PXwDb0ZmaDwvPozcz6nBt6M7M+54bezKzPuaE3M+tzbujNzPrc/wcqSL+2OLGO\nLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa0d9426ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.evaluate([X_train[:100], extract_chars(X_train[:100])], numpy.array(y_train)[:100], cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adding the CRF layer\n",
    "\n",
    "Last, we are going to add the CRF layer on top of everything else.\n",
    "\n",
    "We are using the code originally in [UKPLab EMNLP repository](github.com/uKPLab/emnlp2017-bilstm-cnn-crf) that I'm only copying into this repository to avoid compatibility problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ChainCRF import ChainCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CEmbWEmbBiLSTMCRF(CEmbWEmbBiLSTM):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50, max_char_index=50, char_embedding_type='cnn',\n",
    "                 max_word_length=20, classifier='softmax'):\n",
    "        super(CEmbWEmbBiLSTMCRF, self).__init__(\n",
    "            vocabulary_size, max_sentence_length, labels,\n",
    "            embedding_size, max_char_index, char_embedding_type, max_word_len)\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def add_output_layer(self, layers):\n",
    "        print(layers)\n",
    "        if self.classifier.lower() == 'softmax':\n",
    "            output = TimeDistributed(\n",
    "                Dense(len(self.labels), activation='softmax'),\n",
    "                name=modelName+'_softmax')(layers)\n",
    "            loss_function = 'sparse_categorical_crossentropy'\n",
    "        elif self.classifier.lower() == 'crf':\n",
    "            output = TimeDistributed(Dense(len(self.labels), activation=None),\n",
    "                                     name='hidden_lin_layer')(layers)\n",
    "            crf = ChainCRF(name='crf_layer')\n",
    "            output = crf(output)\n",
    "            loss_function = crf.loss\n",
    "        else:\n",
    "            raise ValueError('The classifier must be softmax of CRF')\n",
    "        return output, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bidirectional_8/concat:0\", shape=(?, ?, 200), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, 70, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_input (InputLayer)         (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, 70, 26, 15)   930         char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 70, 300)      3277200     word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn (TimeDistributed)      (None, 70, 26, 5)    380         char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 70, 300)      0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "char_pooling (TimeDistributed)  (None, 70, 5)        0           char_cnn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 70, 305)      0           dropout_6[0][0]                  \n",
      "                                                                 char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 70, 200)      324800      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_lin_layer (TimeDistribut (None, 70, 17)       3417        bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_layer (ChainCRF)            (None, 70, 17)       323         hidden_lin_layer[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,607,050\n",
      "Trainable params: 329,850\n",
      "Non-trainable params: 3,277,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CEmbWEmbBiLSTMCRF(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=300,\n",
    "    max_char_index=max(char2idx.values()),  # The max possible id for a character\n",
    "    char_embedding_type='cnn', max_word_length=max_word_len,\n",
    "    classifier='crf')\n",
    "model.add_word_embeddings(embeddings_matrix)\n",
    "model.build()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 4s 52ms/step - loss: 192.6944 - acc: 0.5980 - val_loss: 185.4018 - val_acc: 0.3493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0d1adeb70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=[X_train[:100], extract_chars(X_train[:100])],\n",
    "          y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:am_env]",
   "language": "python",
   "name": "conda-env-am_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
