{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# BiLSTM architectures for Sequence Labeling\n",
    "\n",
    "We will be working with the CONLL 2003 dataset, annotated for the task of Named Entity Recognition.\n",
    "\n",
    "The objetive of this notebook is to build different BiLSTM prototype networks for sequence labeling, and apply a very simple attention mechanisms before the recurrent layer. The base model is inpired in [this work](https://www.kaggle.com/gagandeep16/ner-using-bidirectional-lstm), by GaganBhatia. Most of the explanations of the code is in the accompaning slides.\n",
    "\n",
    "You can find two sample datasets directly hosted at UNC, [one](https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.csv) used by the original Kaggle notebook (150M) and a [smaller one](https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.sample.csv) just to play with (14M). If you are running in colab, just run the next cell with the corresponding URL to donwload the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n",
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libcrypto.so.1.0.0: no version information available (required by wget)\r\n",
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libssl.so.1.0.0: no version information available (required by wget)\r\n",
      "wget: /home/milagro/miniconda2/envs/am_env/lib/libssl.so.1.0.0: no version information available (required by wget)\r\n",
      "File `data/data.sample.csv' already there; not retrieving.\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir data; wget -O data/data.sample.csv -nc https://cs.famaf.unc.edu.ar/~mteruel/datasets/tensorflowMeetup/ner.sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gzip\n",
    "import keras\n",
    "import numpy\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "import seaborn\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dataset = pandas.read_csv(\"data/data.sample.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False,\n",
    "                          usecols=['sentence_idx', 'word', 'pos', 'tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let's take a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VBP</td>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBN</td>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TO</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos  sentence_idx           word    tag\n",
       "0  NNS             1      Thousands      O\n",
       "1   IN             1             of      O\n",
       "2  NNS             1  demonstrators      O\n",
       "3  VBP             1           have      O\n",
       "4  VBN             1        marched      O\n",
       "5   IN             1        through      O\n",
       "6  NNP             1         London  B-geo\n",
       "7   TO             1             to      O\n",
       "8   VB             1        protest      O\n",
       "9   DT             1            the      O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class SentenceFactory(object):\n",
    "    \n",
    "    def __init__(self, dataset, tag_preprocess=lambda x: x):\n",
    "        self.dataset = dataset\n",
    "        agg_func = lambda s: [\n",
    "            (w, p, tag_preprocess(t)) \n",
    "            for w, p, t in zip(s[\"word\"].values.tolist(), s['pos'].values.tolist(),\n",
    "                               s[\"tag\"].values.tolist())\n",
    "        ]\n",
    "        grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Thousands', 'NNS', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('demonstrators', 'NNS', 'O'),\n",
       "  ('have', 'VBP', 'O'),\n",
       "  ('marched', 'VBN', 'O'),\n",
       "  ('through', 'IN', 'O'),\n",
       "  ('London', 'NNP', 'B-geo'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('protest', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('war', 'NN', 'O'),\n",
       "  ('in', 'IN', 'O'),\n",
       "  ('Iraq', 'NNP', 'B-geo'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('demand', 'VB', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('withdrawal', 'NN', 'O'),\n",
       "  ('of', 'IN', 'O'),\n",
       "  ('British', 'JJ', 'B-gpe'),\n",
       "  ('troops', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('that', 'DT', 'O'),\n",
       "  ('country', 'NN', 'O'),\n",
       "  ('.', '.', 'O')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances = SentenceFactory(dataset).sentences\n",
    "\n",
    "instances[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 10923\n"
     ]
    }
   ],
   "source": [
    "unique_words = dataset.word.unique()\n",
    "unique_words.sort()\n",
    "unique_words = numpy.append(unique_words, \"ENDPAD\")\n",
    "print('Vocabulary size {}'.format(unique_words.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '\"', '$', ..., '\\x85', '°C', 'ENDPAD'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'B-geo' 'B-gpe' 'B-per' 'I-geo' 'B-org' 'I-org' 'B-tim' 'B-art'\n",
      " 'I-art' 'I-per' 'I-gpe' 'I-tim' 'B-nat' 'B-eve' 'I-eve' 'I-nat']\n",
      "Unique labels 17\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.tag.fillna('O').unique()\n",
    "print(labels)\n",
    "print('Unique labels {}'.format(labels.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Processing the input sequences\n",
    "\n",
    "We need to pad our sequences to the same number of words, so the network can build a tensor from our (variable lenght) sentences. There are workarounds for this step, but maybe for another tutorial. In this case, we choose to use the lenght of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length = dataset.groupby('sentence_idx').word.count().max()\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(unique_words)}\n",
    "labels2idx = {t: i for i, t in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "x_matrix = [[word2idx[w[0]] for w in s] for s in instances]\n",
    "x_matrix = pad_sequences(maxlen=max_sentence_length, sequences=x_matrix,\n",
    "                         padding=\"post\", value=unique_words.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "y = [[labels2idx[w[2]] for w in s] for s in instances]\n",
    "y = pad_sequences(maxlen=max_sentence_length, sequences=y, padding=\"post\", value=labels2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=labels.shape[0]) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_matrix, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "--- \n",
    "\n",
    "# Building the model\n",
    "\n",
    "We build a model with an object oriented interface so we can add and remove layers in sub-classes. This is the vanilla model, which is only using the middle BiLSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import (Bidirectional, concatenate, Conv1D, Dense,\n",
    "                          Dropout, Embedding, GlobalMaxPooling1D, Input,\n",
    "                          LSTM, TimeDistributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class BaseBiLSTM(object):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50):\n",
    "        self.model = None\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.labels = labels\n",
    "        self.n_labels = labels.shape[0]\n",
    "    \n",
    "    def add_input_layer(self):\n",
    "        return Input(shape=(None, ))\n",
    "        \n",
    "    def add_embedding_layer(self, layers):\n",
    "        layers = Embedding(\n",
    "            input_dim=self.vocabulary_size,\n",
    "            output_dim=self.embedding_size,\n",
    "            input_length=self.max_sentence_length)(layers)\n",
    "        return Dropout(0.1)(layers)\n",
    "    \n",
    "    def add_recurrent_layer(self, layers):\n",
    "        layers = Bidirectional(\n",
    "            LSTM(units=100, return_sequences=True,\n",
    "                 recurrent_dropout=0.1))(layers)\n",
    "        return layers\n",
    "    \n",
    "    def add_output_layer(self, layers):\n",
    "        layers = TimeDistributed(\n",
    "            Dense(self.n_labels, activation='softmax'),\n",
    "            name='dense_layer')(layers)\n",
    "        return layers, 'categorical_crossentropy'\n",
    "    \n",
    "    def build(self):\n",
    "        inputs = self.add_input_layer()\n",
    "        layers = self.add_embedding_layer(inputs)\n",
    "        layers = self.add_recurrent_layer(layers)\n",
    "        outputs, loss_function = self.add_output_layer(layers)        \n",
    "        \n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer='adam', loss=loss_function,\n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs, batch_size=32, validation_split=0.2):\n",
    "        if self.model is None:\n",
    "            self.build()\n",
    "        return self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                              validation_split=validation_split, verbose=1)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return numpy.argmax(self.model.predict(X_test), axis=-1)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, cm=False):\n",
    "        predictions = numpy.argmax(self.model.predict(X_test), axis=-1).flatten()\n",
    "        true_labels = numpy.argmax(y_test, axis=-1).flatten()\n",
    "        print(metrics.classification_report(true_labels, predictions,\n",
    "                                            target_names=self.labels))\n",
    "        if cm:\n",
    "            seaborn.heatmap(\n",
    "                metrics.confusion_matrix(true_labels, predictions, labels=range(len(self.labels))),\n",
    "                xticklabels=self.labels, yticklabels=self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 70, 50)            546200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 70, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 70, 200)           120800    \n",
      "_________________________________________________________________\n",
      "dense_layer (TimeDistributed (None, 70, 17)            3417      \n",
      "=================================================================\n",
      "Total params: 670,417\n",
      "Trainable params: 670,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BaseBiLSTM(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=50)\n",
    "model.build()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train on a small portion just to check it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 2s 29ms/step - loss: 2.8120 - acc: 0.5179 - val_loss: 2.7183 - val_acc: 0.9607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dedcb77f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=X_train[:100], y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, with keras is very simple to store and load a model. To save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save(os.path.join('models', 'vanilla-bilstm.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load it back, once the model has been built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.load_weights(os.path.join('models', 'vanilla-bilstm.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "# Adding pre-trained word embeddings\n",
    "\n",
    "To preload the weights of a previous embedding, we only need to add an embedding layer with a pre-set weights. However, the indices of the tokens in our data won't be the same as the ones in the pretrained embeddings. As a result, we need a pretraining step where reorder the embeddings to follow the correct order. This step also allows us to discard the embeddings that are not present in the training dataset, significantly reducing the size of the model in memory. However, have into account this should be done only while you are optimizing the hyperparameters. A real model in production may encounter new words, thus benefitting from all the possible embeddings.\n",
    "\n",
    "This process is pretty straight forward and there are several tutorials online that delve deeper into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocabulary, embeddings_filepath):\n",
    "    \"\"\"Create an embedding matrix using the vectors in embeddings_filepath\n",
    "    and following the order of words given in vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        vocabulary: (dict) map from tokens to consecutive indices.\n",
    "        embeddings_filepath: (str) Direction of the embedding file. This function\n",
    "             is implemented for the Konminos embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        A numpy.matrix with shape (vocabulary_size + 1, embedding_size). The first\n",
    "        embedding (row 0) is a zero embedding. All words in vocabulary not found\n",
    "        in the original embedding matrix are filled with random vectors.\n",
    "    \"\"\"\n",
    "    # TODO We are using the Konminos embeddings in part because they are easy to load\n",
    "    # but you should overwrite this part to fit every particular word embedding to load.\n",
    "    # Warning, this may consume your RAM, depedending on the type of embedding\n",
    "    # you are loading!\n",
    "    if vocabulary is None:\n",
    "        raise ValueError('vocabulary cannot be empty')\n",
    "    original_embeddings = gzip.open(embeddings_filepath, 'rt')\n",
    "    embedding_size = None\n",
    "    embeddings = None\n",
    "    not_found_words = 0\n",
    "    seen_words = []\n",
    "    for line in original_embeddings:\n",
    "        split = line.rstrip().split(\" \")\n",
    "        word = split[0]\n",
    "        if embedding_size is None:  # Only runs first time\n",
    "            embedding_size = len(split) - 1\n",
    "            # We add one more place for the padding token\n",
    "            embeddings = numpy.zeros((len(vocabulary) + 1, embedding_size))\n",
    "\n",
    "        # Assure that all lines in the embeddings file are of the same length\n",
    "        if (len(split) - 1) != embedding_size:\n",
    "            print('ERROR: A line in the embeddings file had more or less '\n",
    "                  'dimensions than expected. Skip token.')\n",
    "            continue\n",
    "\n",
    "        if word in vocabulary:\n",
    "            vector = numpy.array([float(num) for num in split[1:]])\n",
    "            embeddings[vocabulary[word]] = vector  # Save the embedding in the\n",
    "                                                   # correct row\n",
    "            seen_words.append(word)\n",
    "        # If the word is not in vocabulary, we ignore the embedding.                \n",
    "    del original_embeddings\n",
    "    # Calculate which words are still missing\n",
    "    empty_words = numpy.where(~embeddings.any(axis=1))[0]\n",
    "    print('{} out of {} words will be set with a random embedding'.format(\n",
    "        empty_words.shape[0], len(vocabulary)))\n",
    "    # Replace missing words with a random vector (so far they are all zeros)\n",
    "    embeddings[empty_words] = numpy.random.uniform(-0.25, 0.25, (len(empty_words), embedding_size))\n",
    "\n",
    "    # Adding padding token at the end\n",
    "    embeddings[embeddings.shape[0] - 1,:] = numpy.zeros(embedding_size)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the embeddings to avoid calculating them again. After the first time you run this cell, comment this two lines and read the embeddings from disk directly. It is very important to keep a copy of your original word2idx, in case you need to preprocess new data. In this case, we are using always the same dataset and ordering the tokens, so we shouldn't have inconsistencies preprocessing the text more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3667 out of 10923 words will be set with a random embedding\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix = build_embedding_matrix(\n",
    "    vocabulary=word2idx,\n",
    "    embeddings_filepath='../../../am/data/embeddings/komninos_english_embeddings.gz')\n",
    "\n",
    "with open(os.path.join('models', 'komninos_word_embeddings.pickle'), 'wb') as file_:\n",
    "      pickle.dump([embeddings_matrix, word2idx], file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'komninos_word_embeddings.pickle'), 'rb') as file_:\n",
    "      embeddings_matrix, _ = pickle.load(file_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can build our new model with only minimal changes. We add a new method to load the\n",
    "pre-trained matrix and update the corresponding attributes in the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class WEmbBiLSTM(BaseBiLSTM):\n",
    "    \n",
    "    def add_word_embeddings(self, embeddings):\n",
    "        \"\"\"Saves into the model a matrix with the original weights for \n",
    "        the word embeddings.\n",
    "        \n",
    "        MUST BE CALLED BEFORE build, otherwise it has no effect.\n",
    "\n",
    "        Args:\n",
    "            embeddings: (numpy.ndarray) a 2-dimensional matrix with shape\n",
    "                (vocabulary_size, embedding_size).\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_size = embeddings.shape[1]  # Overwrite this value\n",
    "        self.vocabulary_size = embeddings.shape[0]  # Overwrite this value\n",
    "\n",
    "    def add_embedding_layer(self, layers):\n",
    "        if self.embeddings is not None:  # Add the pretrained embeddings\n",
    "            layers = Embedding(\n",
    "                input_dim=self.vocabulary_size, output_dim=self.embedding_size,\n",
    "                weights=[self.embeddings],\n",
    "                trainable=False, input_length=self.max_sentence_length)(layers)\n",
    "        else:  # We use brand new embeddings\n",
    "            layers = Embedding(\n",
    "                input_dim=self.vocabulary_size, output_dim=self.embedding_size,\n",
    "                input_length=self.max_sentence_length)(layers)\n",
    "        return Dropout(0.1)(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 70, 300)           3277200   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 70, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dense_layer (TimeDistributed (None, 70, 17)            3417      \n",
      "=================================================================\n",
      "Total params: 3,601,417\n",
      "Trainable params: 324,217\n",
      "Non-trainable params: 3,277,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = WEmbBiLSTM(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=300)\n",
    "model.add_word_embeddings(embeddings_matrix)\n",
    "model.build()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.575054  ,  0.099223  ,  0.434153  , ..., -0.126422  ,\n",
       "         0.926405  , -0.232181  ],\n",
       "       [ 0.12836123,  0.00913203,  0.07571526, ...,  0.09042007,\n",
       "        -0.06730736,  0.04888196],\n",
       "       [-0.379194  ,  0.044204  ,  0.272724  , ..., -0.188948  ,\n",
       "         0.052879  ,  0.125714  ],\n",
       "       ..., \n",
       "       [ 0.13749349,  0.00245045,  0.0964503 , ..., -0.18370005,\n",
       "        -0.20895696,  0.0238491 ],\n",
       "       [-0.0323528 ,  0.01923495,  0.19879483, ...,  0.09524263,\n",
       "        -0.21764618, -0.21075525],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 2.6705 - acc: 0.5798 - val_loss: 2.3072 - val_acc: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0fc476828>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=X_train[:100], y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "# Adding character embeddings\n",
    "\n",
    "Before adding the character embeddings, we need to assign an index to each char. Once we have done the mapping, we need to create a new input for the character information, and pad all the words to the length of the longest word. Note the char input will be a 3-dimensional tensor, because each word will be translated to a list of characters indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_len = max([len(word) for word in unique_words])\n",
    "max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the words that complicates everything are: ['Pakistani-controlled' 'bigger-than-expected' 'pro-U.S.-immigration'\n",
      " 'Chancellor-designate' 'government-controlled' \"Baku-T'bilisi-Erzerum\"\n",
      " 'multi-million-selling' 'counter-revolutionary' 'internationally-backed'\n",
      " 'internationally-recognized']\n"
     ]
    }
   ],
   "source": [
    "print('And the words that complicates everything are: {}'.format(\n",
    "    unique_words[numpy.argsort([len(word) for word in unique_words])[-10:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'62 unique characters used'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars, counts = numpy.unique([c for c in ''.join(unique_words)], return_counts=True)\n",
    "char_limit = 60\n",
    "char2idx = {c: idx + 1 for idx, c in enumerate(unique_chars[numpy.argsort(counts)[-1*char_limit:]])}\n",
    "char2idx['UNK'] = len(char2idx) + 1\n",
    "char2idx['PAD'] = 0  # This is not needed, it's only for clarification\n",
    "'{} unique characters used'.format(len(char2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def extract_chars(train_matrix, char2idx=char2idx, max_word_len=max_word_len,\n",
    "                  unique_words=unique_words):\n",
    "    \"\"\"Return the character indices for the input matrix.\n",
    "    \n",
    "    Args:\n",
    "        matrix: (array) A 2-dimensional array with word indices. The\n",
    "            expected shape is (batch_size, max_sentence_length)\n",
    "        char2idx: (dict) A map from characters to ids. Unknown characters\n",
    "            will be mapped to the last index, and padding characters will\n",
    "            be mapped to the 0 index.\n",
    "        max_word_len: (int) the size of the longest word\n",
    "        unique_words: (list) list with sorted words, that corresponds\n",
    "            to the ids in train_matrix\n",
    "    \n",
    "    Returns:\n",
    "        A 3-dimensional array with shape (batch_size, max_sentence_length,\n",
    "        max_word_lenght)\n",
    "    \"\"\"\n",
    "    char_matrix = numpy.zeros(train_matrix.shape + (max_word_len,) )\n",
    "    unknown_char_idx = len(char2idx) - 1\n",
    "    for example_idx, sentence in enumerate(train_matrix):\n",
    "        for token_position, word_idx in enumerate(sentence):\n",
    "            if word_idx == len(unique_words):  # Padding token\n",
    "                continue\n",
    "            chars = [char2idx.get(c, unknown_char_idx) for c in unique_words[word_idx]]\n",
    "            char_matrix[example_idx, token_position, :len(chars)] = chars\n",
    "    return char_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 70, 26)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chars(X_train[:5]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally build our model. We need to add one more input and the new character embeddings layer. We build a single model for both types of embeddings to ilustrate the fact that they share a lot of code, but ideally this should be implemented in different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class CEmbWEmbBiLSTM(WEmbBiLSTM):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50, max_char_index=50, char_embedding_type='cnn',\n",
    "                 max_word_length=20):\n",
    "        super(CEmbWEmbBiLSTM, self).__init__(vocabulary_size, max_sentence_length,\n",
    "                                             labels, embedding_size)\n",
    "        self.max_char_index = max_char_index\n",
    "        self.char_embedding_type = char_embedding_type\n",
    "        self.max_word_length = max_word_length\n",
    "    \n",
    "    def add_input_layer(self):\n",
    "        word_input = Input(shape=(self.max_sentence_length,),\n",
    "                           name='word_input')\n",
    "        char_input = Input(\n",
    "            shape=(self.max_sentence_length, self.max_word_length),\n",
    "            name='char_input')\n",
    "        return word_input, char_input\n",
    "    \n",
    "    def add_char_embedding_layer(self, char_input):\n",
    "        \"\"\"Add a convolution for the characters\"\"\"\n",
    "        limit = numpy.sqrt(3.0/self.max_char_index)\n",
    "        char_embedding_size = 15  # TODO define as a parameter\n",
    "        # We initialize the char embeddings randomly, \n",
    "        # including the UNK char in position 0.\n",
    "        self.char_embeddings = numpy.random.uniform(\n",
    "            -limit, limit, (self.max_char_index + 1,\n",
    "                            char_embedding_size))\n",
    "\n",
    "        # We need the TimeDistributed layer to embedd to every word\n",
    "        chars_layer = TimeDistributed(Embedding(\n",
    "            input_dim=self.char_embeddings.shape[0],\n",
    "            output_dim=self.char_embeddings.shape[1],\n",
    "            weights=[self.char_embeddings], trainable=True,\n",
    "            mask_zero=True), name='char_embedding')(char_input)\n",
    "\n",
    "        # Use CNNs for character embeddings from Ma and Hovy, 2016\n",
    "        char_filter_size = 5  # TODO define this as a parameter\n",
    "        char_filter_length = 5  # TODO define this as a parameter\n",
    "        chars_layer = TimeDistributed(\n",
    "            Conv1D(char_filter_size, char_filter_length, padding='same'),\n",
    "            name=\"char_cnn\")(chars_layer)\n",
    "        chars_layer = TimeDistributed(GlobalMaxPooling1D(),\n",
    "                                      name=\"char_pooling\")(chars_layer)\n",
    "        return chars_layer\n",
    "    \n",
    "    def add_embedding_layer(self, layers):\n",
    "        word_input, char_input = layers\n",
    "        # Add word embeddings with the previous method\n",
    "        word_layers = super(CEmbWEmbBiLSTM, self).add_embedding_layer(word_input)\n",
    "        \n",
    "        # Add char embeddings\n",
    "        char_layer = self.add_char_embedding_layer(char_input)\n",
    "        return concatenate([word_layers, char_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, 70, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_input (InputLayer)         (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, 70, 26, 15)   930         char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 70, 300)      3277200     word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn (TimeDistributed)      (None, 70, 26, 5)    380         char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 70, 300)      0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "char_pooling (TimeDistributed)  (None, 70, 5)        0           char_cnn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 70, 305)      0           dropout_5[0][0]                  \n",
      "                                                                 char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 70, 200)      324800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (TimeDistributed)   (None, 70, 17)       3417        bidirectional_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 3,606,727\n",
      "Trainable params: 329,527\n",
      "Non-trainable params: 3,277,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CEmbWEmbBiLSTM(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=300,\n",
    "    max_char_index=max(char2idx.values()),  # The max possible id for a character\n",
    "    char_embedding_type='cnn', max_word_length=max_word_len)\n",
    "model.add_word_embeddings(embeddings_matrix)\n",
    "model.build()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 2.4735 - acc: 0.5582 - val_loss: 1.3482 - val_acc: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1de64c8278>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=[X_train[:100], extract_chars(X_train[:100])],\n",
    "          y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the other model now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CEmbWEmbBiLSTM(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=300,\n",
    "    max_char_index=max(char2idx.values()),  # The max possible id for a character\n",
    "    char_embedding_type='lstm', max_word_length=max_word_len)\n",
    "model.add_word_embeddings(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 4s 45ms/step - loss: 2.3867 - acc: 0.4695 - val_loss: 1.2917 - val_acc: 0.9257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1de4443588>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=[X_train[:100], extract_chars(X_train[:100])],\n",
    "          y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save('models/CEmbLSTMWEmbBiLSTM.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.build()\n",
    "model.model.load_weights('models/CEmbLSTMWEmbBiLSTM.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model, although probably it didn't learn anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milagro/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/milagro/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      0.95      0.95      6656\n",
      "      B-geo       0.00      0.00      0.00        55\n",
      "      B-gpe       0.00      0.00      0.00        34\n",
      "      B-per       0.00      0.00      0.00        49\n",
      "      I-geo       0.00      0.00      0.00         8\n",
      "      B-org       0.01      0.07      0.02        41\n",
      "      I-org       0.00      0.00      0.00        38\n",
      "      B-tim       0.00      0.00      0.00        37\n",
      "      B-art       0.00      0.00      0.00         1\n",
      "      I-art       0.00      0.00      0.00         0\n",
      "      I-per       0.00      0.00      0.00        72\n",
      "      I-gpe       0.00      0.00      0.00         1\n",
      "      I-tim       0.00      0.00      0.00         7\n",
      "      B-nat       0.00      0.00      0.00         1\n",
      "      B-eve       0.00      0.00      0.00         0\n",
      "      I-eve       0.00      0.00      0.00         0\n",
      "      I-nat       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.91      0.91      0.91      7000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEPCAYAAABMTw/iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ddf148ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.evaluate([X_train[:100], extract_chars(X_train[:100])], numpy.array(y_train)[:100], cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adding the CRF layer\n",
    "\n",
    "Last, we are going to add the CRF layer on top of everything else.\n",
    "\n",
    "We are using the code originally in [UKPLab EMNLP repository](github.com/uKPLab/emnlp2017-bilstm-cnn-crf) that I'm only copying into this repository to avoid compatibility problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ChainCRF import ChainCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CEmbWEmbBiLSTMCRF(CEmbWEmbBiLSTM):\n",
    "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
    "                 embedding_size=50, max_char_index=50, char_embedding_type='cnn',\n",
    "                 max_word_length=20, classifier='softmax'):\n",
    "        super(CEmbWEmbBiLSTMCRF, self).__init__(\n",
    "            vocabulary_size, max_sentence_length, labels,\n",
    "            embedding_size, max_char_index, char_embedding_type, max_word_len)\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def add_output_layer(self, layers):\n",
    "        print(layers)\n",
    "        if self.classifier.lower() == 'softmax':\n",
    "            output = TimeDistributed(\n",
    "                Dense(len(self.labels), activation='softmax'),\n",
    "                name=modelName+'_softmax')(layers)\n",
    "            loss_function = 'sparse_categorical_crossentropy'\n",
    "        elif self.classifier.lower() == 'crf':\n",
    "            output = TimeDistributed(Dense(len(self.labels), activation=None),\n",
    "                                     name='hidden_lin_layer')(layers)\n",
    "            crf = ChainCRF(name='crf_layer')\n",
    "            output = crf(output)\n",
    "            loss_function = crf.loss\n",
    "        else:\n",
    "            raise ValueError('The classifier must be softmax of CRF')\n",
    "        return output, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bidirectional_8/concat:0\", shape=(?, ?, 200), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, 70, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_input (InputLayer)         (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, 70, 26, 15)   930         char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 70, 300)      3277200     word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn (TimeDistributed)      (None, 70, 26, 5)    380         char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 70, 300)      0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "char_pooling (TimeDistributed)  (None, 70, 5)        0           char_cnn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 70, 305)      0           dropout_6[0][0]                  \n",
      "                                                                 char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 70, 200)      324800      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_lin_layer (TimeDistribut (None, 70, 17)       3417        bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_layer (ChainCRF)            (None, 70, 17)       323         hidden_lin_layer[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,607,050\n",
      "Trainable params: 329,850\n",
      "Non-trainable params: 3,277,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CEmbWEmbBiLSTMCRF(\n",
    "    vocabulary_size=len(word2idx) + 1, max_sentence_length=max_sentence_length,\n",
    "    labels=labels, embedding_size=300,\n",
    "    max_char_index=max(char2idx.values()),  # The max possible id for a character\n",
    "    char_embedding_type='cnn', max_word_length=max_word_len,\n",
    "    classifier='crf')\n",
    "model.add_word_embeddings(embeddings_matrix)\n",
    "model.build()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 4s 52ms/step - loss: 192.6944 - acc: 0.5980 - val_loss: 185.4018 - val_acc: 0.3493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0d1adeb70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train=[X_train[:100], extract_chars(X_train[:100])],\n",
    "          y_train=numpy.array(y_train)[:100], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:am_env]",
   "language": "python",
   "name": "conda-env-am_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
